---
title: 機械学習の数学入門 勾配降下法を使うワケ
tags:  機械学習 深層学習 数学
categories:  深層学習 数学
author: @buto
status: public
created_at: 2020-03-17 13:05:29 +0900
updated_at: 2020-04-25 21:21:59 +0900
published_at: 2020-03-18 23:08:36 +0900
---
勾配法+機械学習の勉強会に行ってきた！
# 勾配法はトライ&エラー
関数の最適解の求め方は2種類あって、
- **解析的に求める**
    - 数式にして`x = 1`のような正確な答えが求められること
    - 正確な答え（最適解）なので誤差は必ず0
- **トライ＆エラーで求める**
    - 実際の値（学習データ）を関数に代入してみて、その誤差を小さくしていく求め方
    - 勾配降下法はこの方法で最適解を求めている

ニューラルネットでは複雑な（次元が多い）関数になるので、
最適解をいつも解析的に求められるわけじゃない（解析的にできることあるんですか？）
# 重み・バイアスの最適化は勾配降下法が効率的
勾配降下法を使わずに誤差が最小になる重みとバイアスにしようとすると

1. ニューラルネット全体の誤差（二乗誤差）を計算
    1. 学習データ1つ目～最後まで1データずつ予測値と正解ラベルの誤差を計算
        - `誤差 = (正解ラベル[n] - 予測値[n])**2`
    1. 1データずつの誤差を合計　→　ニューラルネット全体の誤差（関数）
1. 誤差を最小にする重みとバイアスを探す
    1. ニューラルネット全体の誤差（関数）から重み、バイアスごとに最小点を探す
        - 誤差を1つずつの重み、バイアスで偏微分して傾きが0の点が最適解
        - ↑の全部の重み、バイアス分を連立方程式にして計算する

ニューラルネットが全結合になっていて、ノード・層数が多かったら辛すぎる！

それで、機械学習・ディープラーニングで最適解求める（予測精度を向上する）には
誤差の勾配（損失関数）の傾きが0の点を探していく勾配降下法が使われる
